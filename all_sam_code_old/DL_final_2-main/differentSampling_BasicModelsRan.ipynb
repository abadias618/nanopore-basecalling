{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q1V5NXhkUbdF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout\n",
    "import keras_tuner as kt\n",
    "from  tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "#import torch\n",
    "#import cv2 as cv2\n",
    "#from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import os\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from PIL import Image\n",
    "#from sklearn.metrics import confusion_matrix, classification_report\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFjowqxudg9D",
    "outputId": "146d151b-dfe5-4aff-ddb7-fb729f6b419e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10015\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for filename in os.listdir(\"../archive/images\"):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        c+=1    \n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIvjlrLfarKr",
    "outputId": "7550b282-53a6-4335-f823-a4e3e935309d"
   },
   "outputs": [],
   "source": [
    "groundTruth = pd.read_csv('../archive/GroundTruth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>MEL</th>\n",
       "      <th>NV</th>\n",
       "      <th>BCC</th>\n",
       "      <th>AKIEC</th>\n",
       "      <th>BKL</th>\n",
       "      <th>DF</th>\n",
       "      <th>VASC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0024306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0024307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0024308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0024309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0024310</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>ISIC_0034316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>ISIC_0034317</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>ISIC_0034318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>ISIC_0034319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>ISIC_0034320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10015 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              image  MEL   NV  BCC  AKIEC  BKL   DF  VASC\n",
       "0      ISIC_0024306  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
       "1      ISIC_0024307  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
       "2      ISIC_0024308  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
       "3      ISIC_0024309  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
       "4      ISIC_0024310  1.0  0.0  0.0    0.0  0.0  0.0   0.0\n",
       "...             ...  ...  ...  ...    ...  ...  ...   ...\n",
       "10010  ISIC_0034316  1.0  0.0  0.0    0.0  0.0  0.0   0.0\n",
       "10011  ISIC_0034317  1.0  0.0  0.0    0.0  0.0  0.0   0.0\n",
       "10012  ISIC_0034318  0.0  0.0  0.0    0.0  1.0  0.0   0.0\n",
       "10013  ISIC_0034319  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
       "10014  ISIC_0034320  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
       "\n",
       "[10015 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10015.0\n",
      "10015\n"
     ]
    }
   ],
   "source": [
    "#Check to see if the categories are mutually exclusive\n",
    "sumRows = groundTruth[[\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]].sum(axis=1) \n",
    "#Add up each row. If mutually exclusive sum of each row should be 1\n",
    "\n",
    "print(sumRows.sum(axis=0))\n",
    "print(len(sumRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRY OVER SAMPLING OTHER LABELS AND UNDER SAMPLING THE NV\n",
    "\n",
    "#First, add a new column to make over/undersampling easier\n",
    "\n",
    "def defineLabel(row):\n",
    "    if row['MEL'] == 1:\n",
    "        val = \"MEL\"\n",
    "    elif row['NV'] ==1:\n",
    "        val = 'NV'\n",
    "    elif row['BCC'] ==1:\n",
    "        val = 'BCC'\n",
    "    elif row['AKIEC'] ==1:\n",
    "        val = 'AKIEC'\n",
    "    elif row['BKL'] ==1:\n",
    "        val = 'BKL'\n",
    "    elif row['DF'] ==1:\n",
    "        val = 'DF'\n",
    "    else:\n",
    "        val = 'VASC'\n",
    "    return val\n",
    "\n",
    "groundTruth['label'] = groundTruth.apply(defineLabel, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEL</th>\n",
       "      <td>115.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NV</th>\n",
       "      <td>115.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BCC</th>\n",
       "      <td>115.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKIEC</th>\n",
       "      <td>115.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BKL</th>\n",
       "      <td>115.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF</th>\n",
       "      <td>115.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VASC</th>\n",
       "      <td>115.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Total  Percent Total\n",
       "MEL    115.0       0.142857\n",
       "NV     115.0       0.142857\n",
       "BCC    115.0       0.142857\n",
       "AKIEC  115.0       0.142857\n",
       "BKL    115.0       0.142857\n",
       "DF     115.0       0.142857\n",
       "VASC   115.0       0.142857"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAKE MULTI CLASS UNDERSAMPLED DATA SET\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "multi_x_under, multi_y_under = rus.fit_resample(groundTruth, groundTruth[\"label\"])\n",
    "multi_x_under = multi_x_under.iloc[:,0:8]\n",
    "\n",
    "sumColumns = pd.DataFrame(multi_x_under[[\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]].sum(axis=0))\n",
    "sumColumns = sumColumns.rename(columns={0: \"Total\"})\n",
    "sumColumns['Percent Total'] = sumColumns['Total'] / len(multi_x_under)\n",
    "sumColumns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>MEL</th>\n",
       "      <th>NV</th>\n",
       "      <th>BCC</th>\n",
       "      <th>AKIEC</th>\n",
       "      <th>BKL</th>\n",
       "      <th>DF</th>\n",
       "      <th>VASC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0030375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0027231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0030826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0024522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0026709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>ISIC_0033969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>ISIC_0024867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>ISIC_0025628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>ISIC_0025596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>ISIC_0033254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>805 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image  MEL   NV  BCC  AKIEC  BKL   DF  VASC\n",
       "0    ISIC_0030375  0.0  0.0  0.0    1.0  0.0  0.0   0.0\n",
       "1    ISIC_0027231  0.0  0.0  0.0    1.0  0.0  0.0   0.0\n",
       "2    ISIC_0030826  0.0  0.0  0.0    1.0  0.0  0.0   0.0\n",
       "3    ISIC_0024522  0.0  0.0  0.0    1.0  0.0  0.0   0.0\n",
       "4    ISIC_0026709  0.0  0.0  0.0    1.0  0.0  0.0   0.0\n",
       "..            ...  ...  ...  ...    ...  ...  ...   ...\n",
       "800  ISIC_0033969  0.0  0.0  0.0    0.0  0.0  0.0   1.0\n",
       "801  ISIC_0024867  0.0  0.0  0.0    0.0  0.0  0.0   1.0\n",
       "802  ISIC_0025628  0.0  0.0  0.0    0.0  0.0  0.0   1.0\n",
       "803  ISIC_0025596  0.0  0.0  0.0    0.0  0.0  0.0   1.0\n",
       "804  ISIC_0033254  0.0  0.0  0.0    0.0  0.0  0.0   1.0\n",
       "\n",
       "[805 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_x_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLE MULTI UNDER DATA\n",
    "multi_under_subset_data = multi_x_under\n",
    "multi_under_TrainData, multi_under_TestData  = train_test_split(multi_under_subset_data, test_size=.3, random_state=42, stratify = multi_under_subset_data[[\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]])\n",
    "multi_under_ValData, multi_under_TestData  = train_test_split(multi_under_TestData, test_size=.5, random_state=42, stratify = multi_under_TestData[[\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]])\n",
    "#READ CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS MULTI UNDER IMAGES TRAIN\n",
    "multi_under_images_train = []\n",
    "\n",
    "for i in multi_under_TrainData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    multi_under_images_train.append(input_arr)\n",
    "    \n",
    "#PROCESS MULTI UNDER IMAGES TEST\n",
    "multi_under_images_test = []\n",
    "\n",
    "for i in multi_under_TestData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    multi_under_images_test.append(input_arr)\n",
    "    \n",
    "#PROCESS MULTI UNDER IMAGES VAL\n",
    "multi_under_images_val = []\n",
    "\n",
    "for i in multi_under_ValData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    multi_under_images_val.append(input_arr)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(563, 8)\n",
      "(121, 8)\n",
      "(121, 8)\n",
      "(563, 450, 600, 3)\n",
      "(121, 450, 600, 3)\n",
      "(121, 450, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "#FINAL UNDER MULTI PROCESSING\n",
    "\n",
    "#Train\n",
    "multi_under_yTrain = np.asarray(multi_under_TrainData.loc[:, multi_under_TrainData.columns != 'image'])\n",
    "print(multi_under_TrainData.shape)\n",
    "\n",
    "#Test\n",
    "multi_under_yTest = np.asarray(multi_under_TestData.loc[:, multi_under_TestData.columns != 'image'])\n",
    "print(multi_under_TestData.shape)\n",
    "\n",
    "#Val\n",
    "multi_under_yVal = np.asarray(multi_under_ValData.loc[:, multi_under_ValData.columns != 'image'])\n",
    "print(multi_under_ValData.shape)\n",
    "\n",
    "#Train\n",
    "multi_under_xTrain = np.asarray(multi_under_images_train).astype('float32') / 255.\n",
    "print(multi_under_xTrain.shape)\n",
    "\n",
    "#Test\n",
    "multi_under_xTest = np.asarray(multi_under_images_test).astype('float32') / 255.\n",
    "print(multi_under_xTest.shape)\n",
    "\n",
    "#Val\n",
    "multi_under_xVal = np.asarray(multi_under_images_val).astype('float32') / 255.\n",
    "print(multi_under_xVal.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEL</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NV</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BCC</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKIEC</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BKL</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VASC</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Total  Percent Total\n",
       "MEL    6705.0       0.142857\n",
       "NV     6705.0       0.142857\n",
       "BCC    6705.0       0.142857\n",
       "AKIEC  6705.0       0.142857\n",
       "BKL    6705.0       0.142857\n",
       "DF     6705.0       0.142857\n",
       "VASC   6705.0       0.142857"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAKE MULTI CLASS OVERSAMPLED DATA SET\n",
    "\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "multi_x_over, multi_y_over = ros.fit_resample(groundTruth, groundTruth[\"label\"])\n",
    "multi_x_over = multi_x_over.iloc[:,0:8]\n",
    "\n",
    "sumColumns = pd.DataFrame(multi_x_over[[\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]].sum(axis=0))\n",
    "sumColumns = sumColumns.rename(columns={0: \"Total\"})\n",
    "sumColumns['Percent Total'] = sumColumns['Total'] / len(multi_x_over)\n",
    "sumColumns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLE MULTI OVER DATA\n",
    "\n",
    "throwAwayData, multi_over_subset_data  = train_test_split(multi_x_over, test_size=1000, random_state=42, stratify = multi_x_over[[\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]])\n",
    "multi_overTrainData, multi_overTestData  = train_test_split(multi_over_subset_data, test_size=.3, random_state=42, stratify = multi_over_subset_data[[\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]])\n",
    "multi_overValData, mulit_overTestData  = train_test_split(multi_overTestData, test_size=.5, random_state=42, stratify = multi_overTestData[[\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"]])\n",
    "\n",
    "#READ CSV\n",
    "\n",
    "#Free up space\n",
    "throwAwayData = 0\n",
    "triple_over_x = 0\n",
    "multi_y_over = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_over_images_train = []\n",
    "\n",
    "for i in multi_overTrainData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    multi_over_images_train.append(input_arr)\n",
    "    \n",
    "#PROCESS MULTI UNDER IMAGES TEST\n",
    "multi_over_images_test = []\n",
    "\n",
    "for i in multi_overTestData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    multi_over_images_test.append(input_arr)\n",
    "    \n",
    "#PROCESS MULTI UNDER IMAGES VAL\n",
    "multi_over_images_val = []\n",
    "\n",
    "for i in multi_overValData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    multi_over_images_val.append(input_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 7)\n",
      "(300, 7)\n",
      "(150, 7)\n",
      "(700, 450, 600, 3)\n",
      "(300, 450, 600, 3)\n",
      "(150, 450, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "#FINAL OVER MULTI PROCESSING\n",
    "\n",
    "#Train\n",
    "multi_over_yTrain = np.asarray(multi_overTrainData.loc[:, multi_overTrainData.columns != 'image'])\n",
    "print(multi_over_yTrain.shape)\n",
    "\n",
    "#Test\n",
    "multi_over_yTest = np.asarray(multi_overTestData.loc[:, multi_overTestData.columns != 'image'])\n",
    "print(multi_over_yTest.shape)\n",
    "\n",
    "#Val\n",
    "multi_over_yVal = np.asarray(multi_overValData.loc[:, multi_overValData.columns != 'image'])\n",
    "print(multi_over_yVal.shape)\n",
    "\n",
    "#Train\n",
    "multi_over_xTrain = np.asarray(multi_over_images_train).astype('float32') / 255.\n",
    "print(multi_over_xTrain.shape)\n",
    "\n",
    "#Test\n",
    "multi_over_xTest = np.asarray(multi_over_images_test).astype('float32') / 255.\n",
    "print(multi_over_xTest.shape)\n",
    "\n",
    "#Val\n",
    "multi_over_xVal = np.asarray(multi_over_images_val).astype('float32') / 255.\n",
    "print(multi_over_xVal.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NV</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.669496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Total  Percent Total\n",
       "NV  6705.0       0.669496"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAKE BINARY DATA SET\n",
    "binaryDf = groundTruth[[\"image\", \"NV\"]]\n",
    "\n",
    "sumColumns = pd.DataFrame(binaryDf[[\"NV\"]].sum(axis=0))\n",
    "sumColumns = sumColumns.rename(columns={0: \"Total\"})\n",
    "sumColumns['Percent Total'] = sumColumns['Total'] / len(groundTruth)\n",
    "sumColumns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NV</th>\n",
       "      <td>3310.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Total  Percent Total\n",
       "NV  3310.0            0.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UNDERSAMPLE BINARY \n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "binary_x_under, binary_y_under = rus.fit_resample(binaryDf, binaryDf[\"NV\"])\n",
    "\n",
    "sumColumns = pd.DataFrame(binary_x_under[[\"NV\"]].sum(axis=0))\n",
    "sumColumns = sumColumns.rename(columns={0: \"Total\"})\n",
    "sumColumns['Percent Total'] = sumColumns['Total'] / len(binary_x_under)\n",
    "sumColumns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLE BINARY DATA\n",
    "\n",
    "throwAwayData, binary_subset_data  = train_test_split(binary_x_under, test_size=805, random_state=42, stratify = binary_x_under[[\"NV\"]])\n",
    "binaryTrainData, binaryTestData  = train_test_split(binary_subset_data, test_size=.3, random_state=42, stratify = binary_subset_data[[\"NV\"]])\n",
    "binaryValData, binaryTestData  = train_test_split(binaryTestData, test_size=.5, random_state=42, stratify = binaryTestData[[\"NV\"]])\n",
    "#READ CSV\n",
    "\n",
    "#Free up space\n",
    "throwAwayData = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS BINARY IMAGES TRAIN\n",
    "binary_images_train = []\n",
    "\n",
    "for i in binaryTrainData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    binary_images_train.append(input_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS BINARY IMAGES TRAIN\n",
    "binary_images_test = []\n",
    "\n",
    "for i in binaryTestData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    binary_images_test.append(input_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS BINARY IMAGES TRAIN\n",
    "binary_images_val = []\n",
    "\n",
    "for i in binaryValData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    binary_images_val.append(input_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(563, 1)\n",
      "(121, 1)\n",
      "(121, 1)\n",
      "(563, 450, 600, 3)\n",
      "(121, 450, 600, 3)\n",
      "(121, 450, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "#FINAL BINARY PROCESSING\n",
    "\n",
    "#Train\n",
    "binary_yTrain = np.asarray(binaryTrainData.loc[:, binaryTrainData.columns != 'image'])\n",
    "print(binary_yTrain.shape)\n",
    "\n",
    "#Test\n",
    "binary_yTest = np.asarray(binaryTestData.loc[:, binaryTestData.columns != 'image'])\n",
    "print(binary_yTest.shape)\n",
    "\n",
    "#Val\n",
    "binary_yVal = np.asarray(binaryValData.loc[:, binaryValData.columns != 'image'])\n",
    "print(binary_yVal.shape)\n",
    "\n",
    "#Train\n",
    "binary_xTrain = np.asarray(binary_images_train).astype('float32') / 255.\n",
    "print(binary_xTrain.shape)\n",
    "\n",
    "#Test\n",
    "binary_xTest = np.asarray(binary_images_test).astype('float32') / 255.\n",
    "print(binary_xTest.shape)\n",
    "\n",
    "#Val\n",
    "binary_xVal = np.asarray(binary_images_val).astype('float32') / 255.\n",
    "print(binary_xVal.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jf/2_pfnrh5615814ryy_6w6dsw0000gn/T/ipykernel_1252/3484292859.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tripleDf['OTHER'] = tripleDf.apply(addOther, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NV</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.669496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTHER</th>\n",
       "      <td>3168.0</td>\n",
       "      <td>0.316326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VASC</th>\n",
       "      <td>142.0</td>\n",
       "      <td>0.014179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Total  Percent Total\n",
       "NV     6705.0       0.669496\n",
       "OTHER  3168.0       0.316326\n",
       "VASC    142.0       0.014179"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAKE THREE OUTCOME DATA SET\n",
    "tripleDf = groundTruth[[\"image\", \"NV\", \"VASC\"]]\n",
    "\n",
    "def addOther(row):\n",
    "    if row.loc['NV'] == 0 and row.loc['VASC']== 0:\n",
    "        val = 1.\n",
    "    else:\n",
    "        val = 0.\n",
    "    return val\n",
    "\n",
    "tripleDf['OTHER'] = tripleDf.apply(addOther, axis=1)\n",
    "\n",
    "sumColumns = pd.DataFrame(tripleDf[[\"NV\",\"OTHER\",\"VASC\"]].sum(axis=0))\n",
    "sumColumns = sumColumns.rename(columns={0: \"Total\"})\n",
    "sumColumns['Percent Total'] = sumColumns['Total'] / len(groundTruth)\n",
    "sumColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jf/2_pfnrh5615814ryy_6w6dsw0000gn/T/ipykernel_1252/2900800026.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tripleDf['LABEL'] = tripleDf.apply(addName, axis=1)\n"
     ]
    }
   ],
   "source": [
    "def addName(row):\n",
    "    if row.loc['NV'] == 1.:\n",
    "        val = \"NV\"\n",
    "    elif row.loc['OTHER'] == 1.:\n",
    "        val = \"OTHER\"\n",
    "    else:\n",
    "        val = \"VASC\"\n",
    "    return val\n",
    "\n",
    "tripleDf['LABEL'] = tripleDf.apply(addName, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "triple_over_x, triple_over_y = ros.fit_resample(tripleDf, tripleDf[\"LABEL\"])\n",
    "\n",
    "triple_over_x = triple_over_x.iloc[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NV</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.669496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTHER</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.669496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VASC</th>\n",
       "      <td>6705.0</td>\n",
       "      <td>0.669496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Total  Percent Total\n",
       "NV     6705.0       0.669496\n",
       "OTHER  6705.0       0.669496\n",
       "VASC   6705.0       0.669496"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OVER SAMPLED\n",
    "sumColumns = pd.DataFrame(triple_over_x[[\"NV\",\"OTHER\",\"VASC\"]].sum(axis=0))\n",
    "sumColumns = sumColumns.rename(columns={0: \"Total\"})\n",
    "sumColumns['Percent Total'] = sumColumns['Total'] / len(groundTruth)\n",
    "sumColumns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "throwAwayData, triple_subset_data  = train_test_split(triple_over_x, test_size=805, random_state=42, stratify = triple_over_x[[\"NV\", \"OTHER\", \"VASC\"]])\n",
    "tripleTrainData, tripleTestData  = train_test_split(triple_subset_data, test_size=.3, random_state=42, stratify = triple_subset_data[[\"NV\", \"OTHER\", \"VASC\"]])\n",
    "tripleValData, tripleTestData  = train_test_split(tripleTestData, test_size=.5, random_state=42, stratify = tripleTestData[[\"NV\", \"OTHER\", \"VASC\"]])\n",
    "\n",
    "#READ CSV\n",
    "\n",
    "#Free up space\n",
    "throwAwayData = 0\n",
    "triple_over_x = 0\n",
    "triple_over_y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS TRIPLE IMAGES TRAIN\n",
    "triple_images_train = []\n",
    "\n",
    "for i in tripleTrainData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    triple_images_train.append(input_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS TRIPLE IMAGES TEST\n",
    "triple_images_test = []\n",
    "\n",
    "for i in tripleTestData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    triple_images_test.append(input_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS TRIPLE IMAGES VAL\n",
    "triple_images_val = []\n",
    "\n",
    "for i in tripleValData.iloc[:,0]: \n",
    "    path=\"../archive/images/\" + i + \".jpg\"\n",
    "    image=tf.keras.preprocessing.image.load_img(\n",
    "        path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    triple_images_val.append(input_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(563, 3)\n",
      "(121, 3)\n",
      "(121, 3)\n",
      "(563, 450, 600, 3)\n",
      "(121, 450, 600, 3)\n",
      "(121, 450, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "#FINAL TRIPLE PROCESSING\n",
    "\n",
    "#Train\n",
    "triple_yTrain = np.asarray(tripleTrainData.loc[:, tripleTrainData.columns != 'image'])\n",
    "print(triple_yTrain.shape)\n",
    "\n",
    "#Test\n",
    "triple_yTest = np.asarray(tripleTestData.loc[:, tripleTestData.columns != 'image'])\n",
    "print(triple_yTest.shape)\n",
    "\n",
    "#Val\n",
    "triple_yVal = np.asarray(tripleValData.loc[:, tripleValData.columns != 'image'])\n",
    "print(triple_yVal.shape)\n",
    "\n",
    "#Train\n",
    "triple_xTrain = np.asarray(triple_images_train).astype('float32') / 255.\n",
    "print(triple_xTrain.shape)\n",
    "\n",
    "#Test\n",
    "triple_xTest = np.asarray(triple_images_test).astype('float32') / 255.\n",
    "print(triple_xTest.shape)\n",
    "\n",
    "#Val\n",
    "triple_xVal = np.asarray(triple_images_val).astype('float32') / 255.\n",
    "print(triple_xVal.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CSVs for Nidhi\n",
    "tripleTrainData.to_csv('tripleTrainData.csv',index=False)\n",
    "tripleValData.to_csv(\"tripleValData.csv\", index=False)\n",
    "tripleTestData.to_csv(\"tripleTestData.csv\", index=False)\n",
    "\n",
    "binaryTrainData.to_csv('binaryTrainData.csv',index=False)\n",
    "binaryValData.to_csv(\"binaryValData.csv\", index=False)\n",
    "binaryTestData.to_csv(\"binaryTestData.csv\", index=False)\n",
    "\n",
    "multi_overTrainData.to_csv('multi_overTrainData.csv',index=False)\n",
    "multi_overTestData.to_csv(\"multi_overTestData.csv\", index=False)\n",
    "multi_overValData.to_csv(\"multi_overValData.csv\", index=False)\n",
    "\n",
    "multi_under_TrainData.to_csv('multi_under_TrainData.csv',index=False)\n",
    "multi_under_TestData.to_csv(\"multi_under_TestData.csv\", index=False)\n",
    "multi_under_ValData.to_csv(\"multi_under_ValData.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 600, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start Building model\n",
    "\n",
    "shape = multi_over_xTest[0].shape\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE ARE ALL THE NAMES OF THE DATA SETS\n",
    "# triple_xTrain\n",
    "# triple_xTest\n",
    "# triple_xVal\n",
    "\n",
    "# triple_yTrain\n",
    "# triple_yTest\n",
    "# triple_yVal\n",
    "\n",
    "# binary_xTrain\n",
    "# binary_xTest\n",
    "# binary_xVal\n",
    "\n",
    "# binary_yTrain\n",
    "# binary_yTest\n",
    "# binary_yVal\n",
    "\n",
    "# multi_under_xTest\n",
    "# multi_under_xTrain\n",
    "# multi_under_xVal\n",
    "\n",
    "# multi_under_yTest\n",
    "# multi_under_yTrain\n",
    "# multi_under_yVal\n",
    "\n",
    "# multi_over_xTrain\n",
    "# multi_over_xTest\n",
    "# multi_over_xVal\n",
    "\n",
    "# multi_over_yTrain\n",
    "# multi_over_yTest\n",
    "# multi_over_yVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dummy model\n",
    "def buildModelMulti():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\"relu\", input_shape=shape))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(7, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "def buildModelTriple():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\"relu\", input_shape=shape))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "def buildModelBinary():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\"relu\", input_shape=shape))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"softmax\"))\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "#                           BINARY MODEL\n",
    "#**********************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 00:40:30.024640: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 446, 596, 32)      2432      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 442, 592, 32)      25632     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 221, 296, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 221, 296, 32)      0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2093312)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               535888128 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 535,916,449\n",
      "Trainable params: 535,916,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "binaryModel = buildModelBinary()\n",
    "binaryModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryModel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 637s 69s/step - loss: 70.9433 - accuracy: 0.4991 - val_loss: 0.6845 - val_accuracy: 0.5041\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 744s 80s/step - loss: 1.2869 - accuracy: 0.4991 - val_loss: 0.9606 - val_accuracy: 0.5041\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 615s 68s/step - loss: 1.5530 - accuracy: 0.4991 - val_loss: 0.6861 - val_accuracy: 0.5041\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 541s 61s/step - loss: 0.6846 - accuracy: 0.4991 - val_loss: 0.6904 - val_accuracy: 0.5041\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 529s 59s/step - loss: 0.7227 - accuracy: 0.4991 - val_loss: 0.6838 - val_accuracy: 0.5041\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 510s 57s/step - loss: 0.6786 - accuracy: 0.4991 - val_loss: 0.6727 - val_accuracy: 0.5041\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 516s 58s/step - loss: 0.6638 - accuracy: 0.4991 - val_loss: 0.6506 - val_accuracy: 0.5041\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 536s 60s/step - loss: 0.6316 - accuracy: 0.4991 - val_loss: 0.6326 - val_accuracy: 0.5041\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 522s 58s/step - loss: 0.5949 - accuracy: 0.4991 - val_loss: 0.6406 - val_accuracy: 0.5041\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 503s 56s/step - loss: 0.5767 - accuracy: 0.4991 - val_loss: 0.6278 - val_accuracy: 0.5041\n"
     ]
    }
   ],
   "source": [
    "#epochs = 15\n",
    "binaryHistory = binaryModel.fit(binary_xTrain, binary_yTrain, epochs=10, batch_size=64, validation_data=(binary_xVal, binary_yVal))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = binaryModel.predict(binary_xTest)\n",
    "binary_yPred = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_yPred\n",
    "#Predicting everything as the same class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "#                           TRIPLE MODEL\n",
    "#**********************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 446, 596, 32)      2432      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 442, 592, 32)      25632     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 221, 296, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 221, 296, 32)      0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2093312)           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               535888128 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 535,916,963\n",
      "Trainable params: 535,916,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tripleModel = buildModelTriple()\n",
    "tripleModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripleModel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 540s 57s/step - loss: 94.5262 - accuracy: 0.3588 - val_loss: 2.1711 - val_accuracy: 0.4793\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 502s 56s/step - loss: 1.8354 - accuracy: 0.5844 - val_loss: 0.8994 - val_accuracy: 0.6198\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 511s 57s/step - loss: 0.8618 - accuracy: 0.6945 - val_loss: 0.8744 - val_accuracy: 0.6942\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 511s 57s/step - loss: 0.7271 - accuracy: 0.7247 - val_loss: 0.6942 - val_accuracy: 0.7355\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 502s 56s/step - loss: 0.5860 - accuracy: 0.7567 - val_loss: 0.6210 - val_accuracy: 0.7355\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 498s 56s/step - loss: 0.4763 - accuracy: 0.8153 - val_loss: 0.5543 - val_accuracy: 0.7190\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 508s 57s/step - loss: 0.3718 - accuracy: 0.8686 - val_loss: 0.4943 - val_accuracy: 0.7521\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 534s 60s/step - loss: 0.3899 - accuracy: 0.8579 - val_loss: 0.7260 - val_accuracy: 0.6446\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 515s 58s/step - loss: 0.3472 - accuracy: 0.8899 - val_loss: 0.5728 - val_accuracy: 0.7190\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 505s 56s/step - loss: 0.2886 - accuracy: 0.9290 - val_loss: 0.5713 - val_accuracy: 0.7273\n"
     ]
    }
   ],
   "source": [
    "tripleHistory = tripleModel.fit(triple_xTrain, triple_yTrain, epochs=10, batch_size=64, validation_data=(triple_xVal, triple_yVal))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tripleModel.predict(triple_xTest)\n",
    "triple_yPred = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 1, 1, 0, 2, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 2,\n",
       "       1, 1, 0, 1, 0, 2, 2, 1, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 2, 2, 2, 0,\n",
       "       2, 2, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "       2, 1, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 2,\n",
       "       0, 0, 0, 1, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_truth\n",
    "#Predicting things as multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35,  0,  5],\n",
       "       [ 3, 36,  2],\n",
       "       [18,  6, 16]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_truth = []\n",
    "for i in triple_yTest:\n",
    "    if np.array_equal(i,[0,0,1]):\n",
    "        triple_truth.append(2)\n",
    "    if np.array_equal(i,[0,1,0]):\n",
    "        triple_truth.append(1)\n",
    "    if np.array_equal(i,[1,0,0]):\n",
    "        triple_truth.append(0)\n",
    "\n",
    "confusion_matrix(triple_truth,triple_yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "#                           MULTI UNDER MODEL\n",
    "#**********************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 446, 596, 32)      2432      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 442, 592, 32)      25632     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 221, 296, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 221, 296, 32)      0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2093312)           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               535888128 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 535,917,991\n",
      "Trainable params: 535,917,991\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "multiModel_under = buildModelMulti()\n",
    "multiModel_under.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiModel_under.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 554s 59s/step - loss: 71.6319 - accuracy: 0.1261 - val_loss: 2.2083 - val_accuracy: 0.1405\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 483s 54s/step - loss: 1.9863 - accuracy: 0.1972 - val_loss: 1.9358 - val_accuracy: 0.1653\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 497s 56s/step - loss: 1.9374 - accuracy: 0.1776 - val_loss: 1.9002 - val_accuracy: 0.2066\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 499s 56s/step - loss: 2.0120 - accuracy: 0.2753 - val_loss: 1.9197 - val_accuracy: 0.1488\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 494s 56s/step - loss: 1.9093 - accuracy: 0.2131 - val_loss: 1.9202 - val_accuracy: 0.1736\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 488s 55s/step - loss: 3.5711 - accuracy: 0.2931 - val_loss: 1.8989 - val_accuracy: 0.1570\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 485s 54s/step - loss: 2.4756 - accuracy: 0.3410 - val_loss: 1.8963 - val_accuracy: 0.1983\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 509s 57s/step - loss: 1.7877 - accuracy: 0.3943 - val_loss: 1.8738 - val_accuracy: 0.2231\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 517s 58s/step - loss: 1.6512 - accuracy: 0.4156 - val_loss: 2.4992 - val_accuracy: 0.2066\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 486s 54s/step - loss: 1.7154 - accuracy: 0.4032 - val_loss: 2.2503 - val_accuracy: 0.2066\n"
     ]
    }
   ],
   "source": [
    "multiHistory = multiModel_under.fit(multi_under_xTrain, multi_under_yTrain, epochs=10, batch_size=64, validation_data=(multi_under_xTest, multi_under_yTest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 5, 5, 0, 0, 0, 6, 6, 5, 3, 0, 6, 6, 5, 6, 5, 4, 0, 3, 5, 6,\n",
       "       3, 3, 0, 3, 1, 3, 0, 0, 6, 5, 6, 4, 5, 6, 3, 0, 0, 6, 6, 6, 3, 3,\n",
       "       6, 2, 6, 0, 0, 0, 3, 0, 3, 0, 0, 6, 2, 3, 6, 0, 3, 0, 6, 3, 0, 6,\n",
       "       6, 6, 5, 6, 0, 3, 0, 6, 3, 2, 3, 6, 5, 2, 5, 3, 2, 5, 6, 6, 4, 3,\n",
       "       5, 2, 6, 0, 0, 6, 6, 6, 0, 2, 6, 0, 6, 3, 6, 6, 6, 0, 2, 3, 0, 0,\n",
       "       3, 0, 4, 5, 0, 6, 5, 0, 6, 0, 6])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_under_predictions = multiModel_under.predict(multi_under_xTest)\n",
    "multi_under_yPred = np.argmax(multi_under_predictions, axis=1)\n",
    "multi_under_yPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  0,  1,  5,  1,  2,  3],\n",
       "       [ 8,  1,  2,  1,  0,  1,  4],\n",
       "       [ 3,  0,  1,  6,  1,  4,  3],\n",
       "       [ 3,  0,  1,  4,  0,  2,  7],\n",
       "       [ 5,  0,  0,  4,  1,  3,  5],\n",
       "       [ 5,  0,  3,  2,  1,  2,  4],\n",
       "       [ 3,  0,  0,  1,  0,  2, 11]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_under_truth = []\n",
    "for i in multi_under_yTest:\n",
    "    if np.array_equal(i,[0,0,0,0,0,0,1]):\n",
    "        multi_under_truth.append(6)\n",
    "    if np.array_equal(i,[0,0,0,0,0,1,0]):\n",
    "        multi_under_truth.append(5)\n",
    "    if np.array_equal(i,[0,0,0,0,1,0,0]):\n",
    "        multi_under_truth.append(4)\n",
    "    if np.array_equal(i,[0,0,0,1,0,0,0]):\n",
    "        multi_under_truth.append(3)\n",
    "    if np.array_equal(i,[0,0,1,0,0,0,0]):\n",
    "        multi_under_truth.append(2)\n",
    "    if np.array_equal(i,[0,1,0,0,0,0,0]):\n",
    "        multi_under_truth.append(1)\n",
    "    if np.array_equal(i,[1,0,0,0,0,0,0]):\n",
    "        multi_under_truth.append(0)\n",
    "\n",
    "confusion_matrix(multi_under_truth,multi_under_yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********************************************************************\n",
    "#                           MULTI OVER MODEL\n",
    "#**********************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiModel_over = buildModelMulti()\n",
    "multiModel_over.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 700s 61s/step - loss: 119.9630 - accuracy: 0.1729 - val_loss: 2.1034 - val_accuracy: 0.2000\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 639s 58s/step - loss: 1.8974 - accuracy: 0.2529 - val_loss: 1.7846 - val_accuracy: 0.3167\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 643s 58s/step - loss: 1.7209 - accuracy: 0.3329 - val_loss: 1.9010 - val_accuracy: 0.3167\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 631s 58s/step - loss: 1.8072 - accuracy: 0.3943 - val_loss: 1.7497 - val_accuracy: 0.3300\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 634s 58s/step - loss: 1.6009 - accuracy: 0.4000 - val_loss: 1.7115 - val_accuracy: 0.3467\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 651s 60s/step - loss: 1.6301 - accuracy: 0.4500 - val_loss: 1.8405 - val_accuracy: 0.2667\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 667s 62s/step - loss: 1.7422 - accuracy: 0.4800 - val_loss: 2.1042 - val_accuracy: 0.3500\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 659s 60s/step - loss: 1.5778 - accuracy: 0.5957 - val_loss: 1.9051 - val_accuracy: 0.3033\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 667s 61s/step - loss: 1.6661 - accuracy: 0.4971 - val_loss: 1.8457 - val_accuracy: 0.3467\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 622s 57s/step - loss: 1.3851 - accuracy: 0.6500 - val_loss: 2.1799 - val_accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "multiHistory_over = multiModel_over.fit(multi_over_xTrain, multi_over_yTrain, epochs=10, batch_size=64, validation_data=(multi_over_xTest, multi_over_yTest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 4, 3, 6, 6, 6, 2, 6, 6, 6, 4, 6, 6, 0, 4, 2, 6, 5, 2, 6, 6, 4,\n",
       "       5, 6, 6, 6, 0, 6, 5, 6, 4, 2, 6, 0, 6, 3, 1, 6, 2, 2, 6, 6, 6, 5,\n",
       "       1, 5, 4, 2, 0, 0, 6, 4, 5, 2, 5, 2, 6, 5, 5, 0, 6, 6, 2, 3, 5, 6,\n",
       "       3, 6, 2, 1, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 3, 6, 6, 6, 5, 6,\n",
       "       4, 6, 6, 0, 6, 6, 6, 6, 5, 6, 5, 0, 6, 6, 6, 2, 0, 6, 4, 6, 6, 6,\n",
       "       5, 6, 2, 6, 5, 6, 0, 6, 6, 2, 6, 2, 6, 4, 6, 6, 6, 5, 3, 4, 4, 3,\n",
       "       6, 6, 3, 5, 4, 6, 2, 4, 5, 6, 5, 5, 0, 3, 4, 0, 1, 4, 2, 4, 5, 6,\n",
       "       1, 6, 0, 2, 5, 1, 6, 5, 6, 0, 6, 0, 6, 0, 5, 0, 2, 2, 6, 4, 6, 5,\n",
       "       6, 5, 5, 6, 6, 5, 4, 6, 1, 6, 3, 6, 2, 2, 6, 6, 3, 5, 5, 2, 1, 6,\n",
       "       5, 4, 6, 6, 4, 5, 6, 6, 6, 6, 5, 6, 6, 0, 6, 5, 5, 1, 6, 0, 2, 2,\n",
       "       2, 5, 2, 6, 2, 6, 4, 0, 5, 3, 3, 6, 6, 6, 4, 0, 6, 0, 5, 1, 0, 0,\n",
       "       5, 1, 6, 0, 4, 6, 6, 3, 3, 6, 3, 5, 1, 5, 3, 5, 6, 6, 6, 4, 4, 5,\n",
       "       0, 6, 6, 5, 6, 6, 6, 6, 3, 4, 2, 6, 4, 6, 2, 6, 6, 5, 5, 6, 1, 6,\n",
       "       6, 2, 6, 6, 6, 0, 6, 6, 0, 6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_over_predictions = multiModel_over.predict(multi_over_xTest)\n",
    "multi_over_yPred = np.argmax(multi_over_predictions, axis=1)\n",
    "multi_over_yPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  3,  3,  1,  7,  2, 16],\n",
       "       [ 5,  7,  1,  0,  1,  3, 26],\n",
       "       [ 1,  0,  9,  7,  7,  9, 10],\n",
       "       [ 3,  1, 10,  6,  4,  3, 16],\n",
       "       [ 7,  1,  5,  4,  5,  8, 13],\n",
       "       [ 1,  1,  3,  0,  4, 21, 13],\n",
       "       [ 0,  0,  1,  0,  0,  0, 41]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_over_truth = []\n",
    "for i in multi_over_yTest:\n",
    "    if np.array_equal(i,[0,0,0,0,0,0,1]):\n",
    "        multi_over_truth.append(6)\n",
    "    if np.array_equal(i,[0,0,0,0,0,1,0]):\n",
    "        multi_over_truth.append(5)\n",
    "    if np.array_equal(i,[0,0,0,0,1,0,0]):\n",
    "        multi_over_truth.append(4)\n",
    "    if np.array_equal(i,[0,0,0,1,0,0,0]):\n",
    "        multi_over_truth.append(3)\n",
    "    if np.array_equal(i,[0,0,1,0,0,0,0]):\n",
    "        multi_over_truth.append(2)\n",
    "    if np.array_equal(i,[0,1,0,0,0,0,0]):\n",
    "        multi_over_truth.append(1)\n",
    "    if np.array_equal(i,[1,0,0,0,0,0,0]):\n",
    "        multi_over_truth.append(0)\n",
    "\n",
    "confusion_matrix(multi_over_truth,multi_over_yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINARY\n",
      "Test loss: 0.5887302160263062\n",
      "Test accuracy: 0.4958677589893341\n",
      "\n",
      "BINARY\n",
      "Test loss: 0.6839745044708252\n",
      "Test accuracy: 0.7190082669258118\n",
      "\n",
      "MULTI CLASS UNDERSAMPLE\n",
      "Test loss: 2.25026535987854\n",
      "Test accuracy: 0.20661157369613647\n",
      "\n",
      "MULTI CLASS OVERSAMPLE\n",
      "Test loss: 2.179945945739746\n",
      "Test accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "#SHOW ALL RESULTS BEFORE CHOSING WHICH DATA SET TO RUN TUNNING ON\n",
    "binaryEval = binaryModel.evaluate(binary_xTest, binary_yTest, verbose=0)\n",
    "print(\"BINARY\")\n",
    "print('Test loss:', binaryEval[0])\n",
    "print('Test accuracy:', binaryEval[1])\n",
    "\n",
    "tripleEval = tripleModel.evaluate(triple_xTest, triple_yTest, verbose=0)\n",
    "print(\"\\nTRIPLE\")\n",
    "print('Test loss:', tripleEval[0])\n",
    "print('Test accuracy:', tripleEval[1])\n",
    "\n",
    "multi_under_eval = multiModel_under.evaluate(multi_under_xTest, multi_under_yTest, verbose=0)\n",
    "print(\"\\nMULTI CLASS UNDERSAMPLE\")\n",
    "print('Test loss:', multi_under_eval[0])\n",
    "print('Test accuracy:', multi_under_eval[1])\n",
    "\n",
    "multi_over_eval = multiModel_over.evaluate(multi_over_xTest, multi_over_yTest, verbose=0)\n",
    "print(\"\\nMULTI CLASS OVERSAMPLE\")\n",
    "print('Test loss:', multi_over_eval[0])\n",
    "print('Test accuracy:', multi_over_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModelTripleKT(hp):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\"relu\", input_shape=shape))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(Dropout(rate=0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "    model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    buildModelMultiKT,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3, \n",
    "    directory=\"test\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 10m 07s]\n",
      "val_accuracy: 0.2666666805744171\n",
      "\n",
      "Best val_accuracy So Far: 0.2666666805744171\n",
      "Total elapsed time: 00h 26m 49s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(multi_over_xTrain, multi_over_yTrain, epochs=3, validation_data=(multi_over_xVal, multi_over_yVal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_over = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_under_yPred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jf/2_pfnrh5615814ryy_6w6dsw0000gn/T/ipykernel_5650/3152772021.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model_over\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_over_xTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmulti_over_yPred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmulti_under_yPred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'multi_under_yPred' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = best_model_over.predict(multi_over_xTest)\n",
    "multi_over_yPred = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 0, 1, 0, 4, 6, 1, 0, 0, 6, 0, 6, 1, 1, 0, 0, 5, 4, 6, 0, 4,\n",
       "       5, 0, 1, 0, 0, 5, 0, 4, 4, 6, 1, 0, 1, 4, 6, 0, 6, 4, 5, 0, 0, 1,\n",
       "       0, 0, 0, 5, 6, 1, 4, 1, 6, 0, 6, 0, 0, 2, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_over_yPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XTEST = []\n",
    "\n",
    "\n",
    "# #Process all the images in the training data set.\n",
    "# for i in x_data_test.iloc[:,0]: #FIXME CHANGE BACK TO TRAINING DF\n",
    "#     path=\"../archive/images/\" + i + \".jpg\"\n",
    "    \n",
    "#     image=tf.keras.preprocessing.image.load_img(\n",
    "#         path, grayscale=False, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n",
    "#     input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "\n",
    "#     XTEST.append(input_arr)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get y-Data ready for model\n",
    "# YTEST = np.asarray(test1.loc[:, trainingDf.columns != 'image'])\n",
    "# # y_train_final = np.asarray(trainingDf.loc[:, test.columns != 'image'])\n",
    "# print(YTEST.shape)\n",
    "\n",
    "\n",
    "# #Get x_Data ready for model\n",
    "# XTEST = np.asarray(XTEST).astype('float32') / 255.\n",
    "# print(XTEST.shape) #x_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = m.predict(XTEST)\n",
    "# y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(YTEST, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YTEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 64  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(810000,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(810000, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final_ae = x_train_final.reshape((len(x_train_final), np.prod(x_train_final.shape[1:])))\n",
    "x_test_final_ae = x_test_final.reshape((len(x_test_final), np.prod(x_test_final.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final_ae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train_final_ae, x_train_final_ae,\n",
    "                epochs=5,\n",
    "                batch_size=64,\n",
    "                shuffle=True,\n",
    "                verbose = True)\n",
    "               # validation_data=(x_test_final, x_test_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test_final_ae)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 9  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test_final_ae[i].reshape(450, 600,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(450, 600,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsed Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounded_accuracy(y_true, y_pred):\n",
    "    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "sparse_l1_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[450, 600, 3]),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(300, activation=\"sigmoid\"),\n",
    "    \n",
    "    keras.layers.ActivityRegularization(l1=1e-3)  # Alternatively, you could add\n",
    "                                                  # activity_regularizer=keras.regularizers.l1(1e-3)\n",
    "                                                  # to the previous layer.\n",
    "])\n",
    "sparse_l1_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[300]),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(300, activation=\"selu\"),\n",
    "    keras.layers.Dense(300, activation=\"selu\"),\n",
    "    keras.layers.Dense(450*600*3, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([450, 600, 3])\n",
    "])\n",
    "sparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])\n",
    "sparse_l1_ae.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "                     metrics=[rounded_accuracy])\n",
    "history = sparse_l1_ae.fit(x_train_final, x_train_final, epochs=5,verbose=True)\n",
    "                        #  validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = sparse_l1_ae.predict(x_test_final)\n",
    "decoded_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1, n + 1):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test_final[i].reshape(450, 600, 3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(450, 600, 3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder for denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new = x_data_final_train.astype('float32') / 255.\n",
    "x_test_new = x_data_final_test.astype('float32') / 255.\n",
    "x_train_new = np.reshape(x_train_new, (len(x_train), 450, 600, 3))\n",
    "x_test_new = np.reshape(x_test_new, (len(x_test), 450, 600, 3))\n",
    "\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train_new + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train_new.shape) \n",
    "x_test_noisy = x_test_new + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test_new.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_noisy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(1, n + 1):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(x_test_noisy[i].reshape(450, 600, 3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "input_img = keras.Input(shape=(450, 600, 3))\n",
    "\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "decoded = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train_noisy, x_train_new,\n",
    "                epochs=10,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy, x_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test_noisy)\n",
    "\n",
    "n = 8\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1, n + 1):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test_noisy[i].reshape(450, 600, 3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(450, 600, 3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Image.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
